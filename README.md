# numgrad: numpy-based auto-differentiating library
<b>numgrad</b> provides you ability to construct your expressions and backpropagate them with pytorch-like interface. \
Already implemented couple of popular  loss functions and neural networks layers. \
For examples of use take a look at tests at <i>nn.py</i>
